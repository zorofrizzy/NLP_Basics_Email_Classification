{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544994a6",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e95fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "\n",
    "# 1. Raw text - model can't distinguish the words.\n",
    "# 2. Tokenize - Split the words so the model knows what to look at\n",
    "# 3. Clean text - remove stopwords, punctuation, lemmatization, etc.\n",
    "# 4. Vectorize - converting word occurence to numeric form\n",
    "# 5. ML - train the model on the vectorized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61636368",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d649255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the porter stemmer - used to stem the words to the root.\n",
    "import nltk\n",
    "#dir(nltk)\n",
    "porter_stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8560610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grows  ---->  grow\n",
      "growing  ---->  grow\n",
      "grower  ---->  grower\n",
      "growing  ---->  grow\n"
     ]
    }
   ],
   "source": [
    "list_words = ['grows', 'growing', 'grower', 'growing']\n",
    "\n",
    "for each_word in list_words:\n",
    "    \n",
    "    print(each_word, \" ----> \", porter_stemmer.stem(each_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, r\"F:\\abc\\abc\\R\")\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7181b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(r\"F:\\abc\\abc\\R\",\"SMSSpamCollection.tsv\"), sep = '\\t')\n",
    "df.columns = ['class', 'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19533980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f208906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9118431a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46497450",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e3f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `nlp` ---> searches for all patterns that contain the word \"nlp\"\n",
    "# `[j-q]` ---> searches for all single charactrers between j & q. (Also including j & q)\n",
    "# `[j-q]+` ---> searches for all charactrers between j & q (both included). Length can  be more than 1\n",
    "# '[0-9]+' ---> searches for all numbers. Example : used to search 2018\n",
    "# `[j-q0-9]+` ---> searches for sequences of chars between j & q OR numbers between 0-9. Ex: nlp2018\n",
    "# '\\s' ----> look for a single white space\n",
    "# \\s+ --> Look for a multiple white spaces.\n",
    "# \\W+ ---> search for non word characters\n",
    "# \\w+ ---> search for one or more WORD characters\n",
    "# \\S+ ---> Search for one or more NON WHITE SPACE characters\n",
    "# S -> space, w -> words :)\n",
    "\n",
    "# '[A-Z]+[0-9]+' ---> Finds multiple characters between A-Z AND numbers 0-9 as well (both)\n",
    "# \\s? ---> this means that the space is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2497adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting a sentence into a list of words\n",
    "re_test = 'This is a made up string to test 2 different regex methods'\n",
    "re_test_messy = 'This    is a made up     string to test 2    different regex methods'\n",
    "re_test_messy_2 = 'This-is-a-made/up.string*to&>>>.test---2\"\"\"\"different`~regex-methods'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17f2f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Split \\s\n",
      "\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', '', '', '', 'is', 'a', 'made', 'up', '', '', '', '', 'string', 'to', 'test', '2', '', '', '', 'different', 'regex', 'methods']\n",
      "['This-is-a-made/up.string*to&>>>.test---2\"\"\"\"different`~regex-methods']\n",
      "\n",
      " using Split \\s+      \n",
      "\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This-is-a-made/up.string*to&>>>.test---2\"\"\"\"different`~regex-methods']\n",
      "\n",
      " using Split \\W+      \n",
      "\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "\n",
      " using findall \\w+      \n",
      "\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n"
     ]
    }
   ],
   "source": [
    "# \\s --> Look for a single white space.\n",
    "print(\"using Split \\s\\n\")\n",
    "print(re.split('\\s', re_test))\n",
    "print(re.split('\\s', re_test_messy))\n",
    "print(re.split('\\s', re_test_messy_2))\n",
    "\n",
    "# \\s+ --> Look for a multiple white spaces.\n",
    "print(\"\\n using Split \\s+      \\n\")\n",
    "print(re.split('\\s+', re_test))\n",
    "print(re.split('\\s+', re_test_messy))\n",
    "print(re.split('\\s+', re_test_messy_2))\n",
    "\n",
    "# \\W+ ---> search for non word characters\n",
    "print(\"\\n using Split \\W+      \\n\")\n",
    "print(re.split('\\W+', re_test))\n",
    "print(re.split('\\W+', re_test_messy))\n",
    "print(re.split('\\W+', re_test_messy_2))\n",
    "\n",
    "# \\W+ ---> search for non word characters\n",
    "print(\"\\n using findall \\w+      \\n\")\n",
    "print(re.findall('\\w+', re_test))\n",
    "print(re.findall('\\w+', re_test_messy))\n",
    "print(re.findall('\\w+', re_test_messy_2))\n",
    "\n",
    "# re.split() and re.findall() are opposite of each other.\n",
    "# re.split() splits on our regex pattern\n",
    "# re.findall() finds our regex pattern.\n",
    "# \\s for re.split() == \\S for re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99872d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I try to follow PEP9 guidelines\n",
      "I try to follow PEP9 guidelines\n",
      "I try to follow PEP9 guidelines\n",
      "I try to follow PEP9 guidelines\n",
      "I try to follow hello guidelines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBreakdown of the Corrected Regex Pattern:\\n\\x08: Word Boundary\\n\\nEnsures that \"PEP\", \"PEEP\", or \"PE\" is a whole word and not part of another word.\\n(PEEP?|\\x08PE\\x08):\\n\\nPEEP?: Matches \"PEEP\" with the last \"P\" being optional.\\n|\\x08PE\\x08: Matches \"PE\" as a whole word.\\nThis part of the pattern matches either \"PEEP\" (with optional \"P\") or \"PE\" as a whole word.\\n(\\\\s?\\\\d+)?:\\n\\n(\\\\s?\\\\d+): Optionally matches a space followed by one or more digits.\\n?: Makes the entire group optional, allowing for an optional space followed by one or more \\ndigits after \"PEEP\", \"PE\", or \"PEP\".\\n\\x08: Word Boundary\\n\\nEnsures that the number is a whole word and not part of another word.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and Replace.\n",
    "\n",
    "pep8 = \"I try to follow PEP8 guidelines\"\n",
    "pep8_2 = \"I try to follow PEP75 guidelines\"\n",
    "pep8_3 = \"I try to follow PEEP8 guidelines\"\n",
    "pep8_4 = \"I try to follow PEEP 8 guidelines\"\n",
    "\n",
    "# Replace the incorrect spellings of PEP8 to PEP9.\n",
    "\n",
    "print(re.sub('[A-Z]+[0-9]+', 'PEP9', pep8))\n",
    "print(re.sub('[A-Z]+[0-9]+', 'PEP9', pep8_2))\n",
    "print(re.sub('[A-Z]+[0-9]+', 'PEP9', pep8_3))\n",
    "print(re.sub(r'\\b(PEEP?|\\bPE\\b)(\\s?\\d+)?\\b', 'PEP9', pep8_4))\n",
    "\n",
    "print(re.sub('[A-Z]+\\s?+[0-9]+','hello', pep8_3))\n",
    "\n",
    "# Explanation of \\b(PEEP?|\\bPE\\b)(\\s?\\d+)?\\b\n",
    "\n",
    "'''\n",
    "Breakdown of the Corrected Regex Pattern:\n",
    "\\b: Word Boundary\n",
    "\n",
    "Ensures that \"PEP\", \"PEEP\", or \"PE\" is a whole word and not part of another word.\n",
    "(PEEP?|\\bPE\\b):\n",
    "\n",
    "PEEP?: Matches \"PEEP\" with the last \"P\" being optional.\n",
    "|\\bPE\\b: Matches \"PE\" as a whole word.\n",
    "This part of the pattern matches either \"PEEP\" (with optional \"P\") or \"PE\" as a whole word.\n",
    "(\\s?\\d+)?:\n",
    "\n",
    "(\\s?\\d+): Optionally matches a space followed by one or more digits.\n",
    "?: Makes the entire group optional, allowing for an optional space followed by one or more \n",
    "digits after \"PEEP\", \"PE\", or \"PEP\".\n",
    "\\b: Word Boundary\n",
    "\n",
    "Ensures that the number is a whole word and not part of another word.\n",
    "'''\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3611a",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a6ef08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc7a74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, 750, pound, prize, 2, claim, easy, call, 087187272008, now1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[ü, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week ...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, buying, something, else, next, week, gave, us, free]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  \\\n",
       "0     spam   \n",
       "1      ham   \n",
       "2      ham   \n",
       "3      ham   \n",
       "4      ham   \n",
       "...    ...   \n",
       "5562  spam   \n",
       "5563   ham   \n",
       "5564   ham   \n",
       "5565   ham   \n",
       "5566   ham   \n",
       "\n",
       "                                                                                                     text  \\\n",
       "0     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                           Nah I don't think he goes to usf, he lives around here though   \n",
       "2                           Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                     I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4     As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "...                                                                                                   ...   \n",
       "5562  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...   \n",
       "5563                                                                 Will ü b going to esplanade fr home?   \n",
       "5564                                            Pity, * was in mood for that. So...any other suggestions?   \n",
       "5565  The guy did some bitching but I acted like i'd be interested in buying something else next week ...   \n",
       "5566                                                                           Rofl. Its true to its name   \n",
       "\n",
       "                                                                                           tokenized_text  \n",
       "0     [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                    [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                                 [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                          [date, sunday]  \n",
       "4     [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  \n",
       "...                                                                                                   ...  \n",
       "5562  [2nd, time, tried, 2, contact, u, u, 750, pound, prize, 2, claim, easy, call, 087187272008, now1...  \n",
       "5563                                                                   [ü, b, going, esplanade, fr, home]  \n",
       "5564                                                                     [pity, mood, soany, suggestions]  \n",
       "5565    [guy, bitching, acted, like, id, interested, buying, something, else, next, week, gave, us, free]  \n",
       "5566                                                                                   [rofl, true, name]  \n",
       "\n",
       "[5567 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(column_data):\n",
    "    column_data = \"\".join([word for word in column_data if word not in string.punctuation])\n",
    "    column_data = column_data.lower()\n",
    "    #tokenize\n",
    "    column_data = re.split(\"\\W+\", column_data)\n",
    "    #remove stopwords\n",
    "    column_data = [word for word in column_data if word not in stopwords]\n",
    "    return column_data\n",
    "    \n",
    "df['tokenized_text'] = df['text'].apply(remove_punctuation)\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb4e6d",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c248267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PORTER STEMMING\n",
    "\n",
    "def my_stemmer(column_data):\n",
    "    column_data = [porter_stemmer.stem(word) for word in column_data]\n",
    "    return column_data\n",
    "\n",
    "df['stemmed'] = df['tokenized_text'].apply(my_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6486ff1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                        tokenized_text  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                               stemmed  \n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867cb3c9",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa01b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - Algo that directly chops off the suffix of the word.\n",
    "# Lemmatization - Always returns a dictionary word. It understands the relations more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b68efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9c10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grows  ---->  grow STEMMED\n",
      "grows  ---->  grows LEMMATIZED\n",
      "\n",
      "growing  ---->  grow STEMMED\n",
      "growing  ---->  growing LEMMATIZED\n",
      "\n",
      "grower  ---->  grower STEMMED\n",
      "grower  ---->  grower LEMMATIZED\n",
      "\n",
      "growing  ---->  grow STEMMED\n",
      "growing  ---->  growing LEMMATIZED\n",
      "\n",
      "goose  ---->  goos STEMMED\n",
      "goose  ---->  goose LEMMATIZED\n",
      "\n",
      "geese  ---->  gees STEMMED\n",
      "geese  ---->  goose LEMMATIZED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_words = ['grows', 'growing', 'grower', 'growing', 'goose', 'geese']\n",
    "\n",
    "for each_word in list_words:\n",
    "    \n",
    "    print(each_word, \" ----> \", porter_stemmer.stem(each_word), \"STEMMED\")\n",
    "    print(each_word, \" ----> \", my_lemmatizer.lemmatize(each_word), \"LEMMATIZED\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d2c6b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                        tokenized_text  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                               stemmed  \\\n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...   \n",
       "\n",
       "                                                                                            lemmatized  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing_function(column_data):\n",
    "    column_data = [my_lemmatizer.lemmatize(word) for word in column_data]\n",
    "    return column_data\n",
    "\n",
    "df['lemmatized'] = df['tokenized_text'].apply(lemmatizing_function)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57326da0",
   "metadata": {},
   "source": [
    "## Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f09ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process of encoding text as integers to create feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b36116e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Count Vectorization:\\n\\nHow it works:\\nCount Vectorization represents the text data by counting the frequency of each word in the document. \\nEach document is represented as a vector where each element corresponds to the count of a particular \\nword in the document.\\n\\nAdvantages:\\n\\nSimple and easy to implement.\\nCaptures the local importance of words within a document.\\n\\nDisadvantages:\\n\\nIgnores the order of words.\\nCan result in high-dimensional sparse vectors, which can be computationally expensive.\\n\\n2. TF-IDF (Term Frequency-Inverse Document Frequency):\\nHow it works:\\nTF-IDF measures the importance of a word in a document relative to a corpus. It considers both the \\nfrequency of the word in the document (TF) and the rarity of the word in the corpus (IDF).\\n\\nTF-IDF(�, �) = TF(�,�)×IDF(�)\\nTF-IDF(t,d)=TF(t,d)×IDF(t)\\n\\nTF (Term Frequency): Frequency of a term in a document.\\nIDF (Inverse Document Frequency): Logarithm of the total number of documents divided by the number \\nof documents containing the term.\\n\\nAdvantages:\\n\\nAccounts for the importance of words in a document relative to the entire corpus.\\nReduces the weight of common words that are not informative.\\n\\nDisadvantages:\\n\\nLike Count Vectorization, it also ignores the order of words.\\nCan be sensitive to the quality of the corpus and the chosen parameters.\\n\\n3. N-grams:\\n\\nHow it works:\\nN-grams are contiguous sequences of n items (words, characters, etc.) from a given sample of text. \\nFor example, in the sentence \"I love machine learning,\" the 2-grams (or bigrams) are \"I love,\" \\n\"love machine,\" and \"machine learning.\"\\n\\nAdvantages:\\n\\nCaptures the local context and order of words.\\nCan capture more semantic meaning compared to individual words.\\n\\nDisadvantages:\\n\\nIncreases the dimensionality of the data.\\nCan result in sparse representations for larger n.\\n\\nSummary:\\nCount Vectorization is a straightforward method that counts the occurrences of words in a document.\\nTF-IDF weights words based on their frequency in a document relative to a corpus, aiming to \\nemphasize informative words.\\n\\nN-grams capture the local context and order of words, providing more semantic meaning but \\npotentially leading to higher dimensionality.\\n\\nChoosing the right text representation technique depends on the specific task, the nature of \\nthe data, and the computational resources available. Often, it\\'s beneficial to experiment with \\ndifferent methods and evaluate their performance to determine the most suitable approach for \\na given problem.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfg\n",
    "\"\"\"\n",
    "1. Count Vectorization:\n",
    "\n",
    "How it works:\n",
    "Count Vectorization represents the text data by counting the frequency of each word in the document. \n",
    "Each document is represented as a vector where each element corresponds to the count of a particular \n",
    "word in the document.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Captures the local importance of words within a document.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Ignores the order of words.\n",
    "Can result in high-dimensional sparse vectors, which can be computationally expensive.\n",
    "\n",
    "2. TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "How it works:\n",
    "TF-IDF measures the importance of a word in a document relative to a corpus. It considers both the \n",
    "frequency of the word in the document (TF) and the rarity of the word in the corpus (IDF).\n",
    "\n",
    "TF-IDF(�, �) = TF(�,�)×IDF(�)\n",
    "TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
    "\n",
    "TF (Term Frequency): Frequency of a term in a document.\n",
    "IDF (Inverse Document Frequency): Logarithm of the total number of documents divided by the number \n",
    "of documents containing the term.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Accounts for the importance of words in a document relative to the entire corpus.\n",
    "Reduces the weight of common words that are not informative.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Like Count Vectorization, it also ignores the order of words.\n",
    "Can be sensitive to the quality of the corpus and the chosen parameters.\n",
    "\n",
    "3. N-grams:\n",
    "\n",
    "How it works:\n",
    "N-grams are contiguous sequences of n items (words, characters, etc.) from a given sample of text. \n",
    "For example, in the sentence \"I love machine learning,\" the 2-grams (or bigrams) are \"I love,\" \n",
    "\"love machine,\" and \"machine learning.\"\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Captures the local context and order of words.\n",
    "Can capture more semantic meaning compared to individual words.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Increases the dimensionality of the data.\n",
    "Can result in sparse representations for larger n.\n",
    "\n",
    "Summary:\n",
    "Count Vectorization is a straightforward method that counts the occurrences of words in a document.\n",
    "TF-IDF weights words based on their frequency in a document relative to a corpus, aiming to \n",
    "emphasize informative words.\n",
    "\n",
    "N-grams capture the local context and order of words, providing more semantic meaning but \n",
    "potentially leading to higher dimensionality.\n",
    "\n",
    "Choosing the right text representation technique depends on the specific task, the nature of \n",
    "the data, and the computational resources available. Often, it's beneficial to experiment with \n",
    "different methods and evaluate their performance to determine the most suitable approach for \n",
    "a given problem.\n",
    "\"\"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a11127",
   "metadata": {},
   "source": [
    "### Count Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cecd557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts  may  text fa  receive entry questionstd txt ratetc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nurungu vettam set callertune caller press 9 copy fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                        tokenized_text  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                               stemmed  \\\n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...   \n",
       "\n",
       "                                                                                            lemmatized  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...   \n",
       "\n",
       "                                                                                     lemmatized_joined  \n",
       "0  free entry 2 wkly comp win fa cup final tkts  may  text fa  receive entry questionstd txt ratetc...  \n",
       "1                                                             nah dont think go usf life around though  \n",
       "2                                                        even brother like speak treat like aid patent  \n",
       "3                                                                                          date sunday  \n",
       "4  per request melle melle oru minnaminunginte nurungu vettam set callertune caller press 9 copy fr...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create function to join lemmatized words in  a column and remove numbers.\n",
    "\n",
    "def join_lemmatized(word_list):\n",
    "    word = \" \".join(word_list)\n",
    "    word = re.sub('[0-9]+\\S+', '', word)\n",
    "    return word\n",
    "\n",
    "df['lemmatized_joined'] = df['lemmatized'].apply(join_lemmatized)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a20fd028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 7763)\n",
      "['aa' 'aah' 'aaniye' ... 'zyada' 'üll' '〨ud']\n"
     ]
    }
   ],
   "source": [
    "# Creates a document-term matrix. Count of occurence of each word.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(df['lemmatized_joined'])\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a35735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5567x7763 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44692 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have our count vectorizer, \n",
    "# it will be represented by a sparse matrix (matrix of 0s mostly) \n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d0bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7753</th>\n",
       "      <th>7754</th>\n",
       "      <th>7755</th>\n",
       "      <th>7756</th>\n",
       "      <th>7757</th>\n",
       "      <th>7758</th>\n",
       "      <th>7759</th>\n",
       "      <th>7760</th>\n",
       "      <th>7761</th>\n",
       "      <th>7762</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5567 rows × 7763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  7753  \\\n",
       "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "5562     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5563     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5564     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5565     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5566     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "      7754  7755  7756  7757  7758  7759  7760  7761  7762  \n",
       "0        0     0     0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5562     0     0     0     0     0     0     0     0     0  \n",
       "5563     0     0     0     0     0     0     0     0     0  \n",
       "5564     0     0     0     0     0     0     0     0     0  \n",
       "5565     0     0     0     0     0     0     0     0     0  \n",
       "5566     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5567 rows x 7763 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read it, we convert it to array\n",
    "# and save those arrays in a df.\n",
    "\n",
    "count_vec_X_df = pd.DataFrame(X_counts.toarray())\n",
    "count_vec_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfa0ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aah  aaniye  aaooooright  aathilove  aathiwhere  ab  abbey  abdomen  \\\n",
       "0   0    0       0            0          0           0   0      0        0   \n",
       "1   0    0       0            0          0           0   0      0        0   \n",
       "2   0    0       0            0          0           0   0      0        0   \n",
       "3   0    0       0            0          0           0   0      0        0   \n",
       "4   0    0       0            0          0           0   0      0        0   \n",
       "\n",
       "   abeg  ...  zero  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  üll  〨ud  \n",
       "0     0  ...     0      0       0    0          0     0     0      0    0    0  \n",
       "1     0  ...     0      0       0    0          0     0     0      0    0    0  \n",
       "2     0  ...     0      0       0    0          0     0     0      0    0    0  \n",
       "3     0  ...     0      0       0    0          0     0     0      0    0    0  \n",
       "4     0  ...     0      0       0    0          0     0     0      0    0    0  \n",
       "\n",
       "[5 rows x 7763 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column names can be assigned back by \n",
    "count_vec_X_df.columns = count_vect.get_feature_names_out()\n",
    "\n",
    "count_vec_X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc78d2",
   "metadata": {},
   "source": [
    "## N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec89fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n combinations of adjacent words are formed in the matrix.\n",
    "# \"nlp is an interesting topic\"\n",
    "# bigram = ['nlp is', 'is an', 'an interesting', 'interesting topic']\n",
    "# trigram = ['nlp is an', 'is an interesting', 'an interesting topic']\n",
    "# 4 gram = ['nlp is an interesting', 'is an interesting topic']\n",
    "\n",
    "# choose n by tuning.\n",
    "# Count vectorizer = n gram (n =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca86b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 29898)\n",
      "['aa exhaust' 'aah bless' 'aah cuddle' ... 'üll submitting' 'üll take'\n",
      " '〨ud evening']\n"
     ]
    }
   ],
   "source": [
    "# ngram also uses the count vectorizer for initialization\n",
    "# ngram_range of (1, 1) means only unigrams, (1, 2) means\n",
    "#     unigrams and bigrams, and (2, 2) means only bigrams. [min, max]\n",
    "\n",
    "ngram_vector = CountVectorizer(ngram_range = (2,2))\n",
    "\n",
    "X_counts = ngram_vector.fit_transform(df['lemmatized_joined'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be61d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa exhaust</th>\n",
       "      <th>aah bless</th>\n",
       "      <th>aah cuddle</th>\n",
       "      <th>aah speak</th>\n",
       "      <th>aaniye pudunga</th>\n",
       "      <th>aaooooright work</th>\n",
       "      <th>aathilove lot</th>\n",
       "      <th>aathiwhere dear</th>\n",
       "      <th>ab sara</th>\n",
       "      <th>abbey happy</th>\n",
       "      <th>...</th>\n",
       "      <th>zoe hit</th>\n",
       "      <th>zoe join</th>\n",
       "      <th>zogtorius staring</th>\n",
       "      <th>zoom cine</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>üll finish</th>\n",
       "      <th>üll submitting</th>\n",
       "      <th>üll take</th>\n",
       "      <th>〨ud evening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa exhaust  aah bless  aah cuddle  aah speak  aaniye pudunga  \\\n",
       "0           0          0           0          0               0   \n",
       "1           0          0           0          0               0   \n",
       "2           0          0           0          0               0   \n",
       "3           0          0           0          0               0   \n",
       "4           0          0           0          0               0   \n",
       "\n",
       "   aaooooright work  aathilove lot  aathiwhere dear  ab sara  abbey happy  \\\n",
       "0                 0              0                0        0            0   \n",
       "1                 0              0                0        0            0   \n",
       "2                 0              0                0        0            0   \n",
       "3                 0              0                0        0            0   \n",
       "4                 0              0                0        0            0   \n",
       "\n",
       "   ...  zoe hit  zoe join  zogtorius staring  zoom cine  zouk nichols  \\\n",
       "0  ...        0         0                  0          0             0   \n",
       "1  ...        0         0                  0          0             0   \n",
       "2  ...        0         0                  0          0             0   \n",
       "3  ...        0         0                  0          0             0   \n",
       "4  ...        0         0                  0          0             0   \n",
       "\n",
       "   zyada kisi  üll finish  üll submitting  üll take  〨ud evening  \n",
       "0           0           0               0         0            0  \n",
       "1           0           0               0         0            0  \n",
       "2           0           0               0         0            0  \n",
       "3           0           0               0         0            0  \n",
       "4           0           0               0         0            0  \n",
       "\n",
       "[5 rows x 29898 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, we will have a sparse matrix. \n",
    "ngram_vec_X_df = pd.DataFrame(X_counts.toarray())\n",
    "ngram_vec_X_df.columns = ngram_vector.get_feature_names_out()\n",
    "ngram_vec_X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3d1f2",
   "metadata": {},
   "source": [
    "## TF - IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "185bb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W(i,j) = tf(i,j) * log(N/df(i))\n",
    "# tf(i,j) = no. of times i occurs in j divided by total number of terms in j\n",
    "# df(i) = no. of documents containing i.\n",
    "# N = total number of documents.\n",
    "\n",
    "# Tells us how important a word is for a message. We get weights.\n",
    "\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "# for \" i like NLP\",\n",
    "# tf(NLP, j) = (no. of occurences of NLP)/(no. of words in message) = 1/3 = 0.33\n",
    "# N = 20\n",
    "# df(NLP) = 1\n",
    "# W(i, j) = 0.33 * log(20/1) = 0.33 * 1.301 = 0.43 (log uses base 10 for this ex. usually uses ln)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "628ecca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 7763)\n",
      "['aa' 'aah' 'aaniye' ... 'zyada' 'üll' '〨ud']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vect.fit_transform(df['lemmatized_joined'])\n",
    "\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a086ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aah  aaniye  aaooooright  aathilove  aathiwhere   ab  abbey  abdomen  \\\n",
       "0  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0      0.0   \n",
       "1  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0      0.0   \n",
       "2  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0      0.0   \n",
       "3  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0      0.0   \n",
       "4  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0      0.0   \n",
       "\n",
       "   abeg  ...  zero  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  üll  〨ud  \n",
       "0   0.0  ...   0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "1   0.0  ...   0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "2   0.0  ...   0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "3   0.0  ...   0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "4   0.0  ...   0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 7763 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_X_df = pd.DataFrame(X_tfidf.toarray())\n",
    "tfidf_vect_X_df.columns = tfidf_vect.get_feature_names_out()\n",
    "tfidf_vect_X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44373715",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959b045",
   "metadata": {},
   "source": [
    "## Creating Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bb54751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_joined</th>\n",
       "      <th>message_length</th>\n",
       "      <th>percent_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts  may  text fa  receive entry questionstd txt ratetc...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "      <td>62</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>date sunday</td>\n",
       "      <td>28</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nurungu vettam set callertune caller press 9 copy fr...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                        tokenized_text  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                               stemmed  \\\n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...   \n",
       "\n",
       "                                                                                            lemmatized  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...   \n",
       "\n",
       "                                                                                     lemmatized_joined  \\\n",
       "0  free entry 2 wkly comp win fa cup final tkts  may  text fa  receive entry questionstd txt ratetc...   \n",
       "1                                                             nah dont think go usf life around though   \n",
       "2                                                        even brother like speak treat like aid patent   \n",
       "3                                                                                          date sunday   \n",
       "4  per request melle melle oru minnaminunginte nurungu vettam set callertune caller press 9 copy fr...   \n",
       "\n",
       "   message_length  percent_punctuation  \n",
       "0             128                 4.69  \n",
       "1              49                 4.08  \n",
       "2              62                 3.23  \n",
       "3              28                 7.14  \n",
       "4             135                 4.44  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature to calculate the length of each message.\n",
    "\n",
    "df['message_length'] = df['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# Feature to calculate what % of text is punctuation\n",
    "\n",
    "def count_punctuation(text):\n",
    "    total = 0\n",
    "    counter = sum([1 for letter in text if letter in string.punctuation])\n",
    "    \n",
    "    return round(counter/(len(text) - text.count(\" \"))*100, 2) # no. of punctuations - no. of spaces\n",
    "\n",
    "df['percent_punctuation'] = df['text'].apply(count_punctuation)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc960a9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa17033",
   "metadata": {},
   "source": [
    "### Random Forest using TFIDF and K FOLD Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3769588d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_length</th>\n",
       "      <th>percent_punctuation</th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_length  percent_punctuation   aa  aah  aaniye  aaooooright  \\\n",
       "0             128                 4.69  0.0  0.0     0.0          0.0   \n",
       "1              49                 4.08  0.0  0.0     0.0          0.0   \n",
       "2              62                 3.23  0.0  0.0     0.0          0.0   \n",
       "3              28                 7.14  0.0  0.0     0.0          0.0   \n",
       "4             135                 4.44  0.0  0.0     0.0          0.0   \n",
       "\n",
       "   aathilove  aathiwhere   ab  abbey  ...  zero  zhong  zindgi  zoe  \\\n",
       "0        0.0         0.0  0.0    0.0  ...   0.0    0.0     0.0  0.0   \n",
       "1        0.0         0.0  0.0    0.0  ...   0.0    0.0     0.0  0.0   \n",
       "2        0.0         0.0  0.0    0.0  ...   0.0    0.0     0.0  0.0   \n",
       "3        0.0         0.0  0.0    0.0  ...   0.0    0.0     0.0  0.0   \n",
       "4        0.0         0.0  0.0    0.0  ...   0.0    0.0     0.0  0.0   \n",
       "\n",
       "   zogtorius  zoom  zouk  zyada  üll  〨ud  \n",
       "0        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "1        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "2        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "3        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "4        0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 7765 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_tfidf = pd.concat([df['message_length'], df['percent_punctuation'], tfidf_vect_X_df], axis = 1)\n",
    "Y_features = df['class']\n",
    "X_features_tfidf.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae3511",
   "metadata": {},
   "source": [
    "### Skip to next heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "423ac397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_build_request_for_signature', '_check_feature_names', '_check_n_features', '_compute_oob_predictions', '_estimator_type', '_get_default_requests', '_get_metadata_request', '_get_oob_predictions', '_get_param_names', '_get_tags', '_make_estimator', '_more_tags', '_parameter_constraints', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_required_parameters', '_set_oob_score_and_attributes', '_validate_X_predict', '_validate_data', '_validate_estimator', '_validate_params', '_validate_y_class_weight', 'apply', 'base_estimator_', 'decision_path', 'feature_importances_', 'fit', 'get_metadata_routing', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_fit_request', 'set_params', 'set_score_request']\n",
      "\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(dir(RandomForestClassifier))\n",
    "print()\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac779c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestClassifier in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  For a comparison between tree-based ensemble models see the example\n",
      " |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      " |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      " |      Note: This parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      " |        split.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool or callable, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
      " |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      " |      custom metric. Only available if `bootstrap=True`.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`Glossary <warm_start>` and\n",
      " |      :ref:`gradient_boosting_warm_start` for details.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator_` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator_` instead.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
      " |      Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
      " |      10_000).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  base_estimator_\n",
      " |      Estimator used to grow the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad79651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# K Fold Cross Validation splits the data into n folds. Then uses each as the test once.\n",
    "# If K = 5, it will train the model 5 times and tell us the accuracy for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "767e9a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97755835, 0.97755835, 0.97484277, 0.9640611 , 0.97574124])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "k_fold = KFold(n_splits = 5)\n",
    "\n",
    "cross_val_score(rf, X_features_tfidf, Y_features, cv = k_fold, scoring = \"accuracy\", n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2218c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the full RF model\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61f31fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, Y_train_tfidf, Y_test_tfidf = train_test_split(X_features_tfidf,\n",
    "                                                                           Y_features,\n",
    "                                                                            test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5409ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6711da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train RF Model : 1.08 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 50, max_depth = 20, n_jobs = -1) \n",
    "rf_model = rf.fit(X_train_tfidf, Y_train_tfidf)\n",
    "\n",
    "print(\"Time taken to train RF Model : {} seconds\".format(round(time.time()-start_time, 2)))\n",
    "# Training with maximum 50 trees.\n",
    "# the max depth of any tree can be 20.\n",
    "# Use all processors in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e2978cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.06081476528508346, 'message_length'), (0.04924806970136537, 'txt'), (0.0484582569145438, 'call'), (0.044125091970542735, 'free'), (0.04262659364571766, 'claim'), (0.02770849167078579, 'mobile'), (0.024401899791802344, 'reply'), (0.023592663647649517, 'text'), (0.02029517194134667, 'service'), (0.01687128687549699, 'pobox')]\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "\n",
    "importance_data = zip(rf_model.feature_importances_, X_train_tfidf.columns)\n",
    "print(sorted(importance_data, reverse = True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eddcc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Precision : 1.0 \n",
      " Recall : 0.62 \n",
      " Accuracy : 1 \n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "y_pred_tfidf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "precision, recall, fscore, support = score(Y_test_tfidf, \n",
    "                                        y_pred_tfidf, \n",
    "                                        pos_label = 'spam', \n",
    "                                        average = 'binary' )\n",
    "\n",
    "print(\" Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(precision, 2),\n",
    "                    round(recall, 2),\n",
    "                    round((y_pred_tfidf==Y_test_tfidf).sum() / len(y_pred_tfidf)), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ad31e",
   "metadata": {},
   "source": [
    "###  GridCVSearch to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c5f889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have X_features_tfidf\n",
    "# Creating X_features_n_grams for n = 2 and X_features_count\n",
    "\n",
    "X_features_n_gram = pd.concat([df['message_length'], df['percent_punctuation'], ngram_vec_X_df],\n",
    "                              axis =1)\n",
    "\n",
    "X_features_count = pd.concat([df['message_length'], df['percent_punctuation'], count_vec_X_df],\n",
    "                              axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d30d9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e742a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are :  {'max_depth': 90, 'n_estimators': 20}\n",
      "Time taken to complete CV Search = 188.11 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.399018</td>\n",
       "      <td>0.269378</td>\n",
       "      <td>0.360723</td>\n",
       "      <td>0.132468</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 20}</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.975749</td>\n",
       "      <td>0.004722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51.249118</td>\n",
       "      <td>2.499986</td>\n",
       "      <td>0.418138</td>\n",
       "      <td>0.044076</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.975749</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.896127</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.476250</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 90}</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.978437</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.975570</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28.091282</td>\n",
       "      <td>0.599448</td>\n",
       "      <td>0.489066</td>\n",
       "      <td>0.124741</td>\n",
       "      <td>None</td>\n",
       "      <td>90</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 90}</td>\n",
       "      <td>0.977558</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.978437</td>\n",
       "      <td>0.975211</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51.376236</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>0.601853</td>\n",
       "      <td>0.057752</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 200}</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>0.971275</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.973773</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "9        6.399018      0.269378         0.360723        0.132468   \n",
       "15      51.249118      2.499986         0.418138        0.044076   \n",
       "10      23.896127      0.999816         0.476250        0.032773   \n",
       "14      28.091282      0.599448         0.489066        0.124741   \n",
       "11      51.376236      0.515474         0.601853        0.057752   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "9               90                 20   \n",
       "15            None                200   \n",
       "10              90                 90   \n",
       "14            None                 90   \n",
       "11              90                200   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "9      {'max_depth': 90, 'n_estimators': 20}           0.981149   \n",
       "15  {'max_depth': None, 'n_estimators': 200}           0.981149   \n",
       "10     {'max_depth': 90, 'n_estimators': 90}           0.982047   \n",
       "14   {'max_depth': None, 'n_estimators': 90}           0.977558   \n",
       "11    {'max_depth': 90, 'n_estimators': 200}           0.981149   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "9            0.973968           0.979335           0.967655   \n",
       "15           0.973968           0.976640           0.971249   \n",
       "10           0.973070           0.978437           0.970350   \n",
       "14           0.973070           0.976640           0.970350   \n",
       "11           0.971275           0.974843           0.967655   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "9            0.976640         0.975749        0.004722                1  \n",
       "15           0.975741         0.975749        0.003267                1  \n",
       "10           0.973944         0.975570        0.004155                3  \n",
       "14           0.978437         0.975211        0.003039                4  \n",
       "11           0.973944         0.973773        0.004455                5  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch CV for TFIDF\n",
    "\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [10,20, 90, 200],\n",
    "    'max_depth' : [10, 30, 90, None]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf, params, cv =5, n_jobs = -1)\n",
    "gs_fit = gs.fit(X_features_tfidf, Y_features)\n",
    "\n",
    "# Printing DF of GridCVSearch results to pick the best value.\n",
    "tfidf_cv_result_df = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', \n",
    "                                                                  ascending = False)[:5]\n",
    "# Storing best params to train model later.\n",
    "best_params_tfidf = gs_fit.best_params_\n",
    "\n",
    "print(\"Best parameters are : \", gs_fit.best_params_)\n",
    "print(\"Time taken to complete CV Search = {} seconds\".format(round(time.time()-start_time, 2)))\n",
    "\n",
    "tfidf_cv_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83c5dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are :  {'max_depth': 90, 'n_estimators': 10}\n",
      "Time taken to complete CV Search = 187.24 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.426046</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.368868</td>\n",
       "      <td>0.064689</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.975210</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.645732</td>\n",
       "      <td>0.223943</td>\n",
       "      <td>0.292020</td>\n",
       "      <td>0.041871</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 20}</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.975210</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.991685</td>\n",
       "      <td>0.511405</td>\n",
       "      <td>0.494475</td>\n",
       "      <td>0.074590</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 90}</td>\n",
       "      <td>0.980251</td>\n",
       "      <td>0.970377</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.974672</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>48.373144</td>\n",
       "      <td>1.908640</td>\n",
       "      <td>0.440557</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.980251</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.974671</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28.689256</td>\n",
       "      <td>0.596009</td>\n",
       "      <td>0.443410</td>\n",
       "      <td>0.050243</td>\n",
       "      <td>None</td>\n",
       "      <td>90</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 90}</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.974671</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8        4.426046      0.285100         0.368868        0.064689   \n",
       "9        6.645732      0.223943         0.292020        0.041871   \n",
       "10      23.991685      0.511405         0.494475        0.074590   \n",
       "15      48.373144      1.908640         0.440557        0.039600   \n",
       "14      28.689256      0.596009         0.443410        0.050243   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                 10   \n",
       "9               90                 20   \n",
       "10              90                 90   \n",
       "15            None                200   \n",
       "14            None                 90   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "8      {'max_depth': 90, 'n_estimators': 10}           0.978456   \n",
       "9      {'max_depth': 90, 'n_estimators': 20}           0.982047   \n",
       "10     {'max_depth': 90, 'n_estimators': 90}           0.980251   \n",
       "15  {'max_depth': None, 'n_estimators': 200}           0.980251   \n",
       "14   {'max_depth': None, 'n_estimators': 90}           0.978456   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "8            0.976661           0.975741           0.970350   \n",
       "9            0.974865           0.974843           0.969452   \n",
       "10           0.970377           0.976640           0.969452   \n",
       "15           0.973968           0.976640           0.968553   \n",
       "14           0.976661           0.975741           0.967655   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "8            0.974843         0.975210        0.002708                1  \n",
       "9            0.974843         0.975210        0.004007                2  \n",
       "10           0.976640         0.974672        0.004113                3  \n",
       "15           0.973944         0.974671        0.003833                4  \n",
       "14           0.974843         0.974671        0.003706                5  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch CV for Count Vectorization\n",
    "\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [10,20, 90, 200],\n",
    "    'max_depth' : [10, 30, 90, None]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf, params, cv =5, n_jobs = -1)\n",
    "gs_fit = gs.fit(X_features_count, Y_features)\n",
    "\n",
    "# Printing DF of GridCVSearch results to pick the best value.\n",
    "count_cv_result_df = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', \n",
    "                                                                  ascending = False)[:5]\n",
    "# Storing best params to train model later.\n",
    "best_params_count_vec = gs_fit.best_params_\n",
    "\n",
    "\n",
    "print(\"Best parameters are : \", gs_fit.best_params_)\n",
    "print(\"Time taken to complete CV Search = {} seconds\".format(round(time.time()-start_time, 2)))\n",
    "\n",
    "count_cv_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28b4c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are :  {'max_depth': None, 'n_estimators': 90}\n",
      "Time taken to complete CV Search = 2208.64 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>66.459179</td>\n",
       "      <td>2.932381</td>\n",
       "      <td>0.886601</td>\n",
       "      <td>0.030912</td>\n",
       "      <td>None</td>\n",
       "      <td>90</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 90}</td>\n",
       "      <td>0.956912</td>\n",
       "      <td>0.941652</td>\n",
       "      <td>0.944295</td>\n",
       "      <td>0.941599</td>\n",
       "      <td>0.940701</td>\n",
       "      <td>0.945032</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>143.466651</td>\n",
       "      <td>4.250837</td>\n",
       "      <td>1.143675</td>\n",
       "      <td>0.073827</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.951526</td>\n",
       "      <td>0.939856</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.941599</td>\n",
       "      <td>0.946990</td>\n",
       "      <td>0.944674</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17.398925</td>\n",
       "      <td>0.794697</td>\n",
       "      <td>0.746531</td>\n",
       "      <td>0.049003</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 20}</td>\n",
       "      <td>0.948833</td>\n",
       "      <td>0.944345</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>0.947889</td>\n",
       "      <td>0.944314</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.558334</td>\n",
       "      <td>0.297997</td>\n",
       "      <td>0.678740</td>\n",
       "      <td>0.027734</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 10}</td>\n",
       "      <td>0.951526</td>\n",
       "      <td>0.938061</td>\n",
       "      <td>0.944295</td>\n",
       "      <td>0.934412</td>\n",
       "      <td>0.946990</td>\n",
       "      <td>0.943057</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>58.530460</td>\n",
       "      <td>0.683217</td>\n",
       "      <td>0.799013</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 200}</td>\n",
       "      <td>0.936266</td>\n",
       "      <td>0.933573</td>\n",
       "      <td>0.930818</td>\n",
       "      <td>0.926325</td>\n",
       "      <td>0.931716</td>\n",
       "      <td>0.931739</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14      66.459179      2.932381         0.886601        0.030912   \n",
       "15     143.466651      4.250837         1.143675        0.073827   \n",
       "13      17.398925      0.794697         0.746531        0.049003   \n",
       "12       9.558334      0.297997         0.678740        0.027734   \n",
       "11      58.530460      0.683217         0.799013        0.017196   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "14            None                 90   \n",
       "15            None                200   \n",
       "13            None                 20   \n",
       "12            None                 10   \n",
       "11              90                200   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "14   {'max_depth': None, 'n_estimators': 90}           0.956912   \n",
       "15  {'max_depth': None, 'n_estimators': 200}           0.951526   \n",
       "13   {'max_depth': None, 'n_estimators': 20}           0.948833   \n",
       "12   {'max_depth': None, 'n_estimators': 10}           0.951526   \n",
       "11    {'max_depth': 90, 'n_estimators': 200}           0.936266   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "14           0.941652           0.944295           0.941599   \n",
       "15           0.939856           0.943396           0.941599   \n",
       "13           0.944345           0.943396           0.937107   \n",
       "12           0.938061           0.944295           0.934412   \n",
       "11           0.933573           0.930818           0.926325   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "14           0.940701         0.945032        0.006060                1  \n",
       "15           0.946990         0.944674        0.004162                2  \n",
       "13           0.947889         0.944314        0.004147                3  \n",
       "12           0.946990         0.943057        0.006139                4  \n",
       "11           0.931716         0.931739        0.003287                5  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch CV for N Gram Vectorization\n",
    "\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [10,20, 90, 200],\n",
    "    'max_depth' : [10, 30, 90, None]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf, params, cv =5, n_jobs = None)\n",
    "gs_fit = gs.fit(X_features_n_gram, Y_features)\n",
    "\n",
    "# Printing DF of GridCVSearch results to pick the best value.\n",
    "ngram_cv_result_df = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', \n",
    "                                                                  ascending = False)[:5]\n",
    "# Storing best params to train model later.\n",
    "best_params_ngram = gs_fit.best_params_\n",
    "\n",
    "print(\"Best parameters are : \", gs_fit.best_params_)\n",
    "print(\"Time taken to complete CV Search = {} seconds\".format(round(time.time()-start_time, 2)))\n",
    "\n",
    "ngram_cv_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722c27b",
   "metadata": {},
   "source": [
    "## training models for all 3 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ab38352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6f23030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X train and Y train and X test and Y test for count vec and ngram\n",
    "\n",
    "X_train_count_vec, X_test_count_vec, Y_train_count_vec, Y_test_count_vec = train_test_split(\n",
    "                                                                            X_features_count,\n",
    "                                                                           Y_features,\n",
    "                                                                            test_size = 0.2)\n",
    "\n",
    "X_train_ngram, X_test_ngram, Y_train_ngram, Y_test_ngram = train_test_split(X_features_n_gram,\n",
    "                                                                           Y_features,\n",
    "                                                                            test_size = 0.2)\n",
    "\n",
    "# Remove\n",
    "try:\n",
    "    x = list(best_params_ngram.values())\n",
    "except:\n",
    "    best_params_ngram = {'max_depth': None, 'n_estimators': 90}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9ad3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tfidf\n",
    "tfidf_rf = RandomForestClassifier(n_estimators = best_params_tfidf.get('n_estimators'),\n",
    "                                  max_depth = best_params_tfidf.get('max_depth'), n_jobs = -1) \n",
    "\n",
    "tfidf_rf_model = tfidf_rf.fit(X_train_tfidf, Y_train_tfidf)\n",
    "\n",
    "# train count vec\n",
    "count_vec_rf = RandomForestClassifier(n_estimators = best_params_count_vec.get('n_estimators'),\n",
    "                                  max_depth = best_params_count_vec.get('max_depth'), n_jobs = -1) \n",
    "\n",
    "count_vec_rf_model = count_vec_rf.fit(X_train_count_vec, Y_train_count_vec)\n",
    "\n",
    "# train ngram\n",
    "ngram_rf = RandomForestClassifier(n_estimators = best_params_ngram.get('n_estimators'),\n",
    "                                  max_depth = best_params_ngram.get('max_depth'), n_jobs = -1) \n",
    "\n",
    "ngram_rf_model = ngram_rf.fit(X_train_ngram, Y_train_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3719e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Precision : 1.0 \n",
      " Recall : 0.83 \n",
      " Accuracy : 0.978 \n",
      "\n",
      "\n",
      "COUNT VECTORIZATION Precision : 0.98 \n",
      " Recall : 0.83 \n",
      " Accuracy : 0.974 \n",
      "\n",
      "\n",
      "NGRAM Precision : 0.95 \n",
      " Recall : 0.69 \n",
      " Accuracy : 0.959 \n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "y_pred_tfidf = tfidf_rf_model.predict(X_test_tfidf)\n",
    "y_pred_count_vec = count_vec_rf_model.predict(X_test_count_vec)\n",
    "y_pred_ngram = ngram_rf_model.predict(X_test_ngram)\n",
    "\n",
    "# precision, recall, fscore, support \n",
    "tfidf_result_list = score(Y_test_tfidf, \n",
    "                        y_pred_tfidf, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "count_vec_result_list = score(Y_test_count_vec, \n",
    "                        y_pred_count_vec, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "\n",
    "ngram_result_list = score(Y_test_ngram, \n",
    "                        y_pred_ngram, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "\n",
    "def accuracy_printer(y_pred, y_test):\n",
    "    return round((y_pred==y_test).sum() / len(y_pred), 3)\n",
    "\n",
    "print(\"TFIDF Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(tfidf_result_list[0], 2),\n",
    "                    round(tfidf_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_tfidf, Y_test_tfidf)))\n",
    "      \n",
    "print(\"\\n\\nCOUNT VECTORIZATION Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(count_vec_result_list[0], 2),\n",
    "                    round(count_vec_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_count_vec, Y_test_count_vec)))\n",
    "      \n",
    "print(\"\\n\\nNGRAM Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(ngram_result_list[0], 2),\n",
    "                    round(ngram_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_ngram, Y_test_ngram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529b5f2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bc0eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a92af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "param = {\n",
    "    \"n_estimators\" : [100,150],\n",
    "    \"max_depth\" : [7,11,15],\n",
    "    \"learning_rate\" : [0.1, 0.15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(gb, param, cv = 5, n_jobs = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61fae2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are :  {'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 150}\n",
      "TFIDF Time taken to complete CV Search = 2161.72 seconds\n"
     ]
    }
   ],
   "source": [
    "#TFIDF\n",
    "\n",
    "start_time = time.time()\n",
    "cv_fit_tfidf = grid_search.fit(X_features_tfidf, Y_features)\n",
    "best_params_tfidf= cv_fit_tfidf.best_params_\n",
    "print(\"Best parameters are : \", cv_fit_tfidf.best_params_)\n",
    "print(\"TFIDF Time taken to complete CV Search = {} seconds\".format(round(time.time()-start_time, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca0d9c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are :  {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 150}\n",
      "COUNT VECT Time taken to complete CV Search = 2061.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# COUNT VECTORIZATION\n",
    "\n",
    "start_time = time.time()\n",
    "cv_fit_count = grid_search.fit(X_features_count, Y_features)\n",
    "best_params_count= cv_fit_count.best_params_\n",
    "print(\"Best parameters are : \", cv_fit_count.best_params_)\n",
    "print(\"COUNT VECT Time taken to complete CV Search = {} seconds\".format(\n",
    "    round(time.time()-start_time, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db41d6aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(gb, param, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m cv_fit_ngram \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_features_n_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m best_params_ngram \u001b[38;5;241m=\u001b[39m cv_fit_ngram\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters are : \u001b[39m\u001b[38;5;124m\"\u001b[39m, cv_fit_ngram\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\zoro_venv\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# N GRAMS\n",
    "\n",
    "grid_search = GridSearchCV(gb, param, cv = 5, n_jobs = 2)\n",
    "\n",
    "start_time = time.time()\n",
    "cv_fit_ngram = grid_search.fit(X_features_n_gram, Y_features)\n",
    "best_params_ngram_boosting = cv_fit_ngram.best_params_\n",
    "print(\"Best parameters are : \", cv_fit_ngram.best_params_)\n",
    "print(\"NGRAM Time taken to complete CV Search = {} seconds\".format(\n",
    "    round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c716e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.15, 7, 150])\n",
      "dict_values([0.1, 11, 150])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(best_params_tfidf.values())\n",
    "except:\n",
    "    best_params_tfidf = {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150}\n",
    "    \n",
    "try:\n",
    "    print(best_params_count.values())\n",
    "except:\n",
    "    best_params_count = {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 150}\n",
    "    \n",
    "try:\n",
    "    print(best_params_ngram_boosting.values())\n",
    "except:\n",
    "    best_params_ngram_boosting = {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50eb1743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Training Time --> 214.906 seconds\n"
     ]
    }
   ],
   "source": [
    "# Model training on best parameters.\n",
    "\n",
    "#TFIDF\n",
    "start_time = time.time()\n",
    "boosting_object = GradientBoostingClassifier(learning_rate = best_params_tfidf.get('learning_rate'),\n",
    "                                            max_depth = best_params_tfidf.get('max_depth'),\n",
    "                                            n_estimators = best_params_tfidf.get('n_estimators'))\n",
    "\n",
    "tfidf_model = boosting_object.fit(X_train_tfidf, Y_train_tfidf)\n",
    "print(\"TFIDF Training Time --> {} seconds\".format(round(time.time()-start_time, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aeb964fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT Training Time --> 152.615 seconds\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorization\n",
    "start_time = time.time()\n",
    "boosting_object = GradientBoostingClassifier(learning_rate = best_params_count.get('learning_rate'),\n",
    "                                            max_depth = best_params_count.get('max_depth'),\n",
    "                                            n_estimators = best_params_count.get('n_estimators'))\n",
    "\n",
    "count2__model = boosting_object.fit(X_train_count_vec, Y_train_count_vec)\n",
    "print(\"COUNT Training Time --> {} seconds\".format(round(time.time()-start_time, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6e8dca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGRAM Training Time --> 536.814 seconds\n"
     ]
    }
   ],
   "source": [
    "# NGRAM Vectorization\n",
    "start_time = time.time()\n",
    "boosting_object = GradientBoostingClassifier(learning_rate = best_params_ngram_boosting.get('learning_rate'),\n",
    "                                            max_depth = best_params_ngram_boosting.get('max_depth'),\n",
    "                                            n_estimators = best_params_ngram_boosting.get('n_estimators'))\n",
    "\n",
    "count_model = boosting_object.fit(X_train_ngram, Y_train_ngram)\n",
    "ngram_model = count_model\n",
    "print(\"NGRAM Training Time --> {} seconds\".format(round(time.time()-start_time, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4596d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Precision : 0.93 \n",
      " Recall : 0.86 \n",
      " Accuracy : 0.973 \n",
      "\n",
      "\n",
      "COUNT VECTORIZATION Precision : 0.94 \n",
      " Recall : 0.87 \n",
      " Accuracy : 0.975 \n",
      "\n",
      "\n",
      "NGRAM Precision : 0.88 \n",
      " Recall : 0.73 \n",
      " Accuracy : 0.956 \n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "ngram_model = count_model\n",
    "y_pred_tfidf = tfidf_model.predict(X_test_tfidf)\n",
    "y_pred_count_vec = count2__model.predict(X_test_count_vec)\n",
    "y_pred_ngram = ngram_model.predict(X_test_ngram)\n",
    "\n",
    "# precision, recall, fscore, support \n",
    "tfidf_result_list = score(Y_test_tfidf, \n",
    "                        y_pred_tfidf, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "count_vec_result_list = score(Y_test_count_vec, \n",
    "                        y_pred_count_vec, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "\n",
    "ngram_result_list = score(Y_test_ngram, \n",
    "                        y_pred_ngram, \n",
    "                        pos_label = 'spam', \n",
    "                        average = 'binary' )\n",
    "\n",
    "def accuracy_printer(y_pred, y_test):\n",
    "    return round((y_pred==y_test).sum() / len(y_pred), 3)\n",
    "\n",
    "print(\"TFIDF Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(tfidf_result_list[0], 2),\n",
    "                    round(tfidf_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_tfidf, Y_test_tfidf)))\n",
    "      \n",
    "print(\"\\n\\nCOUNT VECTORIZATION Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(count_vec_result_list[0], 2),\n",
    "                    round(count_vec_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_count_vec, Y_test_count_vec)))\n",
    "      \n",
    "print(\"\\n\\nNGRAM Precision : {} \\n Recall : {} \\n Accuracy : {} \"\n",
    "      .format(round(ngram_result_list[0], 2),\n",
    "                    round(ngram_result_list[1], 2),\n",
    "                    accuracy_printer(y_pred_ngram, Y_test_ngram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf08705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
